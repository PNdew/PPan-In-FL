{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60lUU5V2Ctlw"
      },
      "outputs": [],
      "source": [
        "!pip install flwr torch torchvision\n",
        "!pip install -q flwr[\"simulation\"] tensorflow\n",
        "!pip install -q flwr_datasets[\"vision\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import flwr as fl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Dict, Union, Optional, Callable\n",
        "from flwr.common import Parameters, Scalar, NDArrays\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.server.client_manager import ClientManager\n",
        "from flwr.common import (\n",
        "    EvaluateIns,\n",
        "    EvaluateRes,\n",
        "    FitIns,\n",
        "\n",
        "    FitRes,\n",
        "    Parameters,\n",
        "    ndarrays_to_parameters,\n",
        "    parameters_to_ndarrays,\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "from flwr_datasets import FederatedDataset\n",
        "from flwr_datasets.partitioner import DirichletPartitioner\n",
        "from flwr_datasets.visualization import plot_label_distributions"
      ],
      "metadata": {
        "id": "NcATJHctCw0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLIENTS = 10\n",
        "BATCH_SIZE = 16\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LEARNING_RATE = 0.001\n",
        "PRIVACY_WEIGHT = 0.001\n",
        "NUM_ROUNDS = 15\n",
        "NOISE_SCALE = 0.01"
      ],
      "metadata": {
        "id": "4DDL453iDO9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "RESULTS_DIR = \"results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "def save_metric_to_txt(round_number, metric_name, metric_value, phase=\"train\"):\n",
        "    \"\"\"Lưu một tiêu chí cụ thể vào một tệp riêng biệt\"\"\"\n",
        "    filename = os.path.join(RESULTS_DIR, f\"{metric_name}_{phase}.txt\")\n",
        "    with open(filename, \"a\") as f:  # Mở file ở chế độ append để ghi thêm dữ liệu\n",
        "        f.write(f\"Round {round_number}: {metric_value}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Function to split MNIST using Dirichlet\n",
        "def split_mnist_dirichlet_flwr(num_clients=NUM_CLIENTS, alpha=0.5, seed=42):\n",
        "    partitioner = DirichletPartitioner(\n",
        "        num_partitions=num_clients, partition_by=\"label\", alpha=alpha, seed=seed\n",
        "    )\n",
        "    fds = FederatedDataset(dataset=\"mnist\", partitioners={\"train\": partitioner})\n",
        "    federated_data = {f\"client_{i}\": fds.load_partition(i) for i in range(num_clients)}\n",
        "    return fds, federated_data  # Return both fds and federated_data\n",
        "\n",
        "# Load and visualize data\n",
        "fds, federated_data = split_mnist_dirichlet_flwr()  # Get both fds and federated_data\n",
        "plot_label_distributions(fds.partitioners[\"train\"], label_name=\"label\")\n",
        "\n",
        "# Function to create DataLoader for each client\n",
        "def get_dataloader(client_data, batch_size=BATCH_SIZE):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()  # Chuyển ảnh PIL thành tensor có giá trị trong khoảng [0,1]\n",
        "    ])\n",
        "\n",
        "    # Chuyển đổi danh sách ảnh từ PIL.Image thành tensor\n",
        "    x_tensor = torch.stack([transform(img) for img in client_data[\"image\"]])\n",
        "    y_tensor = torch.tensor(client_data[\"label\"], dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(x_tensor, y_tensor)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define Neural Network Model\n",
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define PPAN Encoder and Adversary\n",
        "class PPAN_Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x))).detach()\n",
        "\n",
        "class PPAN_Adversary(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(PPAN_Adversary, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define PrivacyMechanism with encrypt and decrypt functions\n",
        "class PrivacyMechanism(nn.Module):\n",
        "    def __init__(self, input_dim, noise_scale=NOISE_SCALE):\n",
        "        super().__init__()\n",
        "        self.encoder = PPAN_Encoder(input_dim, 64)\n",
        "        self.adversary = PPAN_Adversary(input_dim, 64)\n",
        "        self.noise_scale = noise_scale\n",
        "\n",
        "    def encrypt(self, x):\n",
        "        generated = self.encoder(x)\n",
        "        if self.training:\n",
        "            noise = torch.randn_like(generated) * self.noise_scale\n",
        "            generated = generated + noise\n",
        "        return generated\n",
        "\n",
        "    def decrypt(self, encrypted):\n",
        "        decoded = self.adversary(encrypted)\n",
        "        return decoded\n",
        "\n",
        "    def forward(self, x):\n",
        "        encrypted = self.encrypt(x)\n",
        "        decoded = self.decrypt(encrypted)\n",
        "        return encrypted, decoded\n",
        "\n",
        "# Compute Privacy Leakage using mutual_info_regression\n",
        "def compute_privacy_leakage(encrypted_weights, original_weights):\n",
        "    encrypted_weights = np.array(encrypted_weights)  # Chuyển đổi sang numpy array\n",
        "    original_weights = np.array(original_weights)  # Chuyển đổi sang numpy array\n",
        "\n",
        "    encrypted_2d = encrypted_weights.reshape(-1, 1)\n",
        "    original_2d = original_weights.reshape(-1, 1)\n",
        "\n",
        "    min_length = min(len(encrypted_2d), len(original_2d))\n",
        "    encrypted_2d = encrypted_2d[:min_length]\n",
        "    original_2d = original_2d[:min_length]\n",
        "\n",
        "    try:\n",
        "        mi_score = mutual_info_regression(encrypted_2d, original_2d.ravel())[0]\n",
        "    except ValueError:\n",
        "        mi_score = 0.0\n",
        "\n",
        "    return mi_score\n",
        "\n",
        "# Compute Distortion\n",
        "def compute_distortion(original_weights, encrypted_weights):\n",
        "    original_weights = np.array(original_weights)\n",
        "    encrypted_weights = np.array(encrypted_weights)\n",
        "    return np.mean((original_weights - encrypted_weights) ** 2)\n",
        "\n",
        "# Evaluate model accuracy on test set\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "# Define PrivacyClient for Federated Learning\n",
        "class PrivacyClient(fl.client.NumPyClient):\n",
        "    def __init__(self, model, train_loader, test_loader):\n",
        "        self.model = model.to(DEVICE)\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.param_shapes = [p.shape for p in self.model.parameters()]\n",
        "        self.total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        self.privacy_mech = PrivacyMechanism(self.total_params, noise_scale=NOISE_SCALE).to(DEVICE)\n",
        "        self.optimizer = optim.Adam(\n",
        "            list(self.model.parameters()) + list(self.privacy_mech.parameters()),\n",
        "            lr=LEARNING_RATE\n",
        "        )\n",
        "\n",
        "    def get_parameters(self, config=None):\n",
        "        \"\"\"Trả về tham số của mô hình dưới dạng danh sách NumPy arrays\"\"\"\n",
        "        return [p.detach().cpu().numpy() for p in self.model.parameters()]\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        \"\"\"Cập nhật mô hình với danh sách NumPy arrays\"\"\"\n",
        "        ndarray_params = [torch.tensor(p, dtype=torch.float32, device=DEVICE) for p in parameters]\n",
        "        params_dict = zip(self.model.state_dict().keys(), ndarray_params)\n",
        "        state_dict = {k: v for k, v in params_dict}\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def fit(self, parameters: NDArrays, config: Dict[str, Scalar]) -> Tuple[NDArrays, int, Dict[str, Scalar]]:\n",
        "        self.set_parameters(parameters)\n",
        "        self.model.train()\n",
        "        self.privacy_mech.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0  # Tổng số mẫu thực tế\n",
        "\n",
        "        for images, labels in self.train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(images)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * labels.size(0)  # Nhân với số lượng mẫu trong batch\n",
        "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "            total += labels.size(0)  # Cập nhật tổng số mẫu\n",
        "\n",
        "        # Chia loss theo tổng số mẫu thay vì số batch\n",
        "        avg_loss = total_loss / total\n",
        "        accuracy = correct / total  # Tính chính xác theo số mẫu\n",
        "\n",
        "        # Get model parameters and apply privacy mechanism\n",
        "        with torch.no_grad():\n",
        "            params = [p.detach().cpu().numpy() for p in self.model.parameters()]\n",
        "            flat_params = np.concatenate([p.flatten() for p in params])\n",
        "            flat_params_tensor = torch.tensor(flat_params, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "            encrypted_params = self.privacy_mech.encrypt(flat_params_tensor)\n",
        "            encrypted_np = encrypted_params.detach().cpu().numpy()\n",
        "\n",
        "            privacy_leakage = float(compute_privacy_leakage(encrypted_np, flat_params))\n",
        "            distortion = float(compute_distortion(flat_params, encrypted_np.flatten()))\n",
        "\n",
        "        round_number = int(config.get(\"round\", 0))\n",
        "\n",
        "        # Return tuple with correct type signature\n",
        "        return params, len(self.train_loader.dataset), {\n",
        "            \"loss\": float(avg_loss),\n",
        "            \"accuracy\": float(accuracy),\n",
        "            \"privacy_leakage\": privacy_leakage,\n",
        "            \"distortion\": distortion\n",
        "        }\n",
        "\n",
        "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]) -> Tuple[float, int, Dict[str, Scalar]]:\n",
        "        self.set_parameters(parameters)\n",
        "        self.model.eval()\n",
        "\n",
        "        total_loss, correct = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in self.test_loader:\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = self.model(images)\n",
        "                loss = F.cross_entropy(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                predicted = outputs.argmax(dim=1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "            flat_params = torch.cat([p.view(-1) for p in self.model.parameters()]).unsqueeze(0).to(DEVICE)\n",
        "            encrypted_params = self.privacy_mech.encrypt(flat_params)\n",
        "            encrypted_np = np.array(encrypted_params.cpu().numpy().flatten(), dtype=np.float32)\n",
        "            original_np = flat_params.cpu().numpy().flatten()\n",
        "\n",
        "            privacy_leakage = compute_privacy_leakage(encrypted_np, original_np)\n",
        "            distortion = compute_distortion(original_np, encrypted_np)\n",
        "\n",
        "        avg_loss = total_loss / len(self.test_loader)\n",
        "        accuracy = correct / len(self.test_loader.dataset)\n",
        "\n",
        "        round_number = int(config.get(\"round\", 0))\n",
        "        save_metric_to_file(\"eval_loss\", avg_loss, round_number)\n",
        "        save_metric_to_file(\"eval_accuracy\", accuracy, round_number)\n",
        "        save_metric_to_file(\"eval_privacy_leakage\", privacy_leakage, round_number)\n",
        "        save_metric_to_file(\"eval_distortion\", distortion, round_number)\n",
        "\n",
        "        return avg_loss, len(self.test_loader.dataset), {\n",
        "            \"accuracy\": float(accuracy),\n",
        "            \"privacy_leakage\": float(privacy_leakage),\n",
        "            \"distortion\": float(distortion)\n",
        "        }\n",
        "def aggregate_weighted_parameters(results: List[Tuple[NDArrays, int]]) -> NDArrays:\n",
        "    \"\"\"Aggregate model parameters using weighted average.\"\"\"\n",
        "    # Calculate total number of examples used during training\n",
        "    total_examples = sum(num_examples for _, num_examples in results)\n",
        "\n",
        "    if total_examples == 0:\n",
        "        return None\n",
        "\n",
        "    # Get parameter shape from first result\n",
        "    params_shape = [param.shape for param in results[0][0]]\n",
        "    weighted_params = [np.zeros_like(param) for param in results[0][0]]\n",
        "\n",
        "    # Calculate weighted parameters\n",
        "    for parameters, num_examples in results:\n",
        "        weight = num_examples / total_examples\n",
        "        for i, param in enumerate(parameters):\n",
        "            weighted_params[i] += param * weight\n",
        "\n",
        "    return weighted_params\n",
        "\n",
        "class FedAvg_Privacy(fl.server.strategy.FedAvg):\n",
        "    \"\"\"Federated Averaging with Privacy strategy.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        fraction_fit: float = 1.0,\n",
        "        fraction_evaluate: float = 1.0,\n",
        "        min_fit_clients: int = 2,\n",
        "        min_evaluate_clients: int = 2,\n",
        "        min_available_clients: int = 2,\n",
        "        evaluate_fn: Optional[\n",
        "            Callable[[int, NDArrays, Dict[str, Scalar]],\n",
        "                    Optional[Tuple[float, Dict[str, Scalar]]]]\n",
        "        ] = None,\n",
        "        on_fit_config_fn: Optional[Callable[[int], Dict[str, Scalar]]] = None,\n",
        "        on_evaluate_config_fn: Optional[Callable[[int], Dict[str, Scalar]]] = None,\n",
        "        accept_failures: bool = True,\n",
        "        initial_parameters: Optional[Parameters] = None,\n",
        "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
        "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
        "        noise_scale: float = NOISE_SCALE,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize FedAvg with Privacy strategy.\"\"\"\n",
        "        super().__init__(\n",
        "            fraction_fit=fraction_fit,\n",
        "            fraction_evaluate=fraction_evaluate,\n",
        "            min_fit_clients=min_fit_clients,\n",
        "            min_evaluate_clients=min_evaluate_clients,\n",
        "            min_available_clients=min_available_clients,\n",
        "            evaluate_fn=evaluate_fn,\n",
        "            on_fit_config_fn=on_fit_config_fn,\n",
        "            on_evaluate_config_fn=on_evaluate_config_fn,\n",
        "            accept_failures=accept_failures,\n",
        "            initial_parameters=initial_parameters,\n",
        "            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n",
        "            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
        "        )\n",
        "        self.noise_scale = noise_scale\n",
        "\n",
        "    def aggregate_fit(\n",
        "        self,\n",
        "        server_round: int,\n",
        "        results: list[tuple[ClientProxy, FitRes]],\n",
        "        failures: list[BaseException],\n",
        "    ) -> tuple[Optional[Parameters], dict[str, Scalar]]:\n",
        "        \"\"\"Aggregate fit results using weighted average with privacy.\"\"\"\n",
        "        if not results:\n",
        "            return None, {}\n",
        "\n",
        "        # Do not aggregate if there are failures and failures are not accepted\n",
        "        if not self.accept_failures and failures:\n",
        "            return None, {}\n",
        "\n",
        "        # Call parent's aggregate_fit to get aggregated parameters\n",
        "        aggregated_result = super().aggregate_fit(server_round, results, failures)\n",
        "        if aggregated_result is None:\n",
        "            return None, {}\n",
        "\n",
        "        parameters_aggregated, metrics = aggregated_result\n",
        "\n",
        "        # Convert parameters to ndarrays\n",
        "        ndarrays = parameters_to_ndarrays(parameters_aggregated)\n",
        "\n",
        "        # Add noise to parameters for privacy\n",
        "        parameters_noised = [\n",
        "            p + np.random.normal(0, self.noise_scale, p.shape)\n",
        "            for p in ndarrays\n",
        "        ]\n",
        "\n",
        "        # Aggregate metrics\n",
        "        metrics_aggregated = {}\n",
        "        if self.fit_metrics_aggregation_fn:\n",
        "            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n",
        "            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n",
        "        # Tính loss, accuracy, privacy leakage, distortion trên server\n",
        "        avg_loss = np.mean([r.metrics[\"loss\"] for _, r in results])\n",
        "        avg_accuracy = np.mean([r.metrics[\"accuracy\"] for _, r in results])\n",
        "        avg_privacy_leakage = np.mean([r.metrics[\"privacy_leakage\"] for _, r in results])\n",
        "        avg_distortion = np.mean([r.metrics[\"distortion\"] for _, r in results])\n",
        "\n",
        "        # Lưu lại kết quả trên server\n",
        "        save_server_metric(server_round, \"loss\", avg_loss, phase=\"train\")\n",
        "        save_server_metric(server_round, \"accuracy\", avg_accuracy, phase=\"train\")\n",
        "        save_server_metric(server_round, \"privacy_leakage\", avg_privacy_leakage, phase=\"train\")\n",
        "        save_server_metric(server_round, \"distortion\", avg_distortion, phase=\"train\")\n",
        "\n",
        "        # Convert back to Parameters and return\n",
        "        return ndarrays_to_parameters(parameters_noised), metrics_aggregated\n",
        "\n",
        "\n",
        "# Config functions for fit and evaluate\n",
        "def fit_config(server_round: int) -> Dict[str, str]:\n",
        "    config = {\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"round\": server_round,\n",
        "    }\n",
        "    return config\n",
        "\n",
        "def evaluate_config(server_round: int) -> Dict[str, str]:\n",
        "    config = {\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"round\": server_round,\n",
        "    }\n",
        "    return config\n",
        "\n",
        "def aggregate_fit_metrics(metrics: List[Tuple[int, Dict[str, Scalar]]]) -> Dict[str, Scalar]:\n",
        "    aggregated_metrics = {}\n",
        "    for _, client_metrics in metrics:\n",
        "        for k, v in client_metrics.items():\n",
        "            aggregated_metrics[k] = aggregated_metrics.get(k, 0) + v\n",
        "    for k in aggregated_metrics:\n",
        "        aggregated_metrics[k] /= len(metrics)\n",
        "    return aggregated_metrics\n",
        "\n",
        "# Federated Learning Simulation\n",
        "def client_fn(context: Context) -> fl.client.Client:\n",
        "    \"\"\"Tạo một Flower client đại diện cho một tổ chức.\"\"\"\n",
        "    partition_id = context.node_config[\"partition-id\"]\n",
        "\n",
        "    # Kiểm tra nếu partition_id hợp lệ\n",
        "    if f\"client_{partition_id}\" not in federated_data:\n",
        "        raise ValueError(f\"Client ID {partition_id} không tồn tại trong dữ liệu!\")\n",
        "    client_data = federated_data[f\"client_{partition_id}\"]\n",
        "    train_loader = get_dataloader(client_data)\n",
        "    test_loader = get_dataloader(client_data)\n",
        "    model = MNISTModel()\n",
        "\n",
        "    # Trả về client\n",
        "    return PrivacyClient(model, train_loader, test_loader).to_client()\n",
        "\n",
        "def main():\n",
        "    strategy = FedAvg_Privacy(\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=1.0,\n",
        "        min_fit_clients=NUM_CLIENTS,\n",
        "        min_evaluate_clients=NUM_CLIENTS,\n",
        "        min_available_clients=NUM_CLIENTS,\n",
        "        on_fit_config_fn=fit_config,\n",
        "        on_evaluate_config_fn=evaluate_config,\n",
        "        fit_metrics_aggregation_fn=aggregate_fit_metrics,\n",
        "        noise_scale=NOISE_SCALE,\n",
        "    )\n",
        "    fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=NUM_CLIENTS,\n",
        "        config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
        "        strategy=strategy,\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "5CadxqA4DEiI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}