{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, List, Tuple\n",
        "import tensorflow as tf\n",
        "import flwr as fl\n",
        "import numpy as np\n",
        "from flwr.common import Metrics\n",
        "from flwr.simulation.ray_transport.utils import enable_tf_gpu_growth\n",
        "from flwr_datasets import FederatedDataset\n",
        "from flwr_datasets.partitioner import DirichletPartitioner\n",
        "from flwr.server.strategy import DPFedAvgFixed\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from typing import Optional, Union\n",
        "import flwr as fl\n",
        "from flwr.common import Parameters, NDArrays\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.common.typing import Config\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from flwr.common import EvaluateIns, EvaluateRes, FitIns, FitRes, Parameters, Scalar\n",
        "from flwr.common.dp import add_gaussian_noise\n",
        "from flwr.common.logger import warn_deprecated_feature\n",
        "from flwr.common.parameter import ndarrays_to_parameters, parameters_to_ndarrays\n",
        "from flwr.server.client_manager import ClientManager\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.server.strategy import DPFedAvgFixed\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from typing import Dict, List, Optional\n",
        "import random\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.server.criterion import Criterion\n",
        "import threading\n",
        "\n",
        "\n",
        "\n",
        "# Constants\n",
        "NUM_CLIENTS = 10\n",
        "K_CLIENTS = 5\n",
        "BATCH_SIZE = 16\n",
        "NUM_ROUNDS = 1000\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Privacy parameters\n",
        "INITIAL_EPSILON = 2.0\n",
        "MIN_EPSILON = 0.1\n",
        "MAX_EPSILON = 5.0\n",
        "TARGET_ACCURACY = 0.85\n",
        "ADJUST_RATE = 0.05\n",
        "NOISE_CLIP = 0.1\n",
        "WINDOW_SIZE = 3\n",
        "\n",
        "\n",
        "def write_to_file(filename, data):\n",
        "    \"\"\"Ghi dữ liệu vào file, mỗi dòng là một giá trị mới.\"\"\"\n",
        "    with open(filename, \"a\") as f:\n",
        "        f.write(f\"{data}\\n\")\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Improved CNN architecture\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "\n",
        "class AdaptiveLDP:\n",
        "    def __init__(self, epsilon=INITIAL_EPSILON):\n",
        "        self.epsilon = epsilon\n",
        "        self.accuracy_history = deque(maxlen=WINDOW_SIZE)\n",
        "        self.noise_history = deque(maxlen=WINDOW_SIZE)\n",
        "        self.leakage_history = deque(maxlen=WINDOW_SIZE)\n",
        "        self.current_noise = 0.0\n",
        "        self.total_noise_added = 0.0\n",
        "        self.noise_samples = 0\n",
        "\n",
        "    def add_noise(self, data: torch.Tensor) -> torch.Tensor:\n",
        "        sensitivity = min(torch.max(torch.abs(data)).item(), 1.0)\n",
        "        scale = min(sensitivity / max(self.epsilon, MIN_EPSILON), NOISE_CLIP)\n",
        "        noise = torch.tensor(np.random.laplace(0, scale, data.shape), dtype=data.dtype, device=data.device)\n",
        "        self.current_noise = torch.mean(torch.abs(noise)).item()\n",
        "        self.noise_history.append(self.current_noise)\n",
        "        self.total_noise_added += self.current_noise\n",
        "        self.noise_samples += 1\n",
        "        return torch.clamp(data + noise, -1.0, 1.0)\n",
        "\n",
        "    def compute_privacy_leakage(self, original: torch.Tensor, noisy: torch.Tensor) -> float:\n",
        "        original_np = original.cpu().numpy().flatten()\n",
        "        noisy_np = noisy.cpu().numpy().flatten()\n",
        "        if len(original_np) > 10:\n",
        "            leakage = mutual_info_regression(original_np.reshape(-1, 1), noisy_np)\n",
        "            return float(leakage[0])\n",
        "        return 0.0\n",
        "\n",
        "    def adjust_epsilon(self, accuracy: float, leakage: float):\n",
        "        self.accuracy_history.append(accuracy)\n",
        "        self.leakage_history.append(leakage)\n",
        "        if len(self.accuracy_history) >= WINDOW_SIZE:\n",
        "            avg_accuracy = np.mean(self.accuracy_history)\n",
        "            avg_leakage = np.mean(self.leakage_history)\n",
        "            accuracy_diff = TARGET_ACCURACY - avg_accuracy\n",
        "            leakage_penalty = 0.1 * avg_leakage\n",
        "            delta = ADJUST_RATE * (accuracy_diff - leakage_penalty)\n",
        "            new_epsilon = self.epsilon * (1.0 + delta)\n",
        "            self.epsilon = np.clip(new_epsilon, MIN_EPSILON, MAX_EPSILON)\n",
        "\n",
        "class PrivacyClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid: str, model: nn.Module, train_loader: DataLoader):\n",
        "        self.cid = cid\n",
        "        self.model = model.to(DEVICE)\n",
        "        self.train_loader = train_loader\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.ldp = AdaptiveLDP()\n",
        "        self.metrics_history = []\n",
        "\n",
        "    def fit(self, parameters: NDArrays, config: Config):\n",
        "        self.model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        privacy_leakage_values = []\n",
        "\n",
        "        for data, target in self.train_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            noisy_data = self.ldp.add_noise(data)\n",
        "            leakage = self.ldp.compute_privacy_leakage(data, noisy_data)\n",
        "            privacy_leakage_values.append(leakage)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(noisy_data)\n",
        "            loss = self.criterion(output, target)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (output.argmax(dim=1) == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "        accuracy = correct / total\n",
        "        avg_loss = total_loss / len(self.train_loader)\n",
        "        avg_leakage = np.mean(privacy_leakage_values)\n",
        "        self.ldp.adjust_epsilon(accuracy, avg_leakage)\n",
        "\n",
        "        metrics = {\"loss\": avg_loss, \"accuracy\": accuracy, \"epsilon\": self.ldp.epsilon, \"leakage\": avg_leakage}\n",
        "        self.metrics_history.append(metrics)\n",
        "        return self.get_parameters({}), total, metrics\n",
        "\n",
        "class FedAvg_Privacy(fl.server.strategy.FedAvg):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.metrics_history = []\n",
        "\n",
        "    def aggregate_fit(self, server_round, results, failures):\n",
        "        aggregated = super().aggregate_fit(server_round, results, failures)\n",
        "        if aggregated is None:\n",
        "            return None, {}\n",
        "        parameters, _ = aggregated\n",
        "        total_samples = sum(fit_res.num_examples for _, fit_res in results)\n",
        "        metrics = {\n",
        "            \"round\": server_round,\n",
        "            \"accuracy\": sum(fit_res.metrics[\"accuracy\"] * fit_res.num_examples for _, fit_res in results) / total_samples,\n",
        "            \"epsilon\": np.mean([fit_res.metrics[\"epsilon\"] for _, fit_res in results]),\n",
        "            \"leakage\": np.mean([fit_res.metrics[\"leakage\"] for _, fit_res in results])\n",
        "        }\n",
        "        write_to_file(\"loss.txt\", metrics[\"loss\"])\n",
        "        write_to_file(\"accuracy.txt\", metrics[\"accuracy\"])\n",
        "        write_to_file(\"epsilon.txt\", metrics[\"epsilon\"])\n",
        "        write_to_file(\"leakage.txt\", metrics[\"leakage\"])\n",
        "        self.metrics_history.append(metrics)\n",
        "        return parameters, metrics\n",
        "\n",
        "def plot_training_metrics(history):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot([m[\"accuracy\"] for m in history])\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot([m[\"epsilon\"] for m in history])\n",
        "    plt.title(\"Epsilon\")\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot([m[\"leakage\"] for m in history])\n",
        "    plt.title(\"Privacy Leakage\")\n",
        "    plt.show()\n",
        "\n",
        "def split_mnist_dirichlet_flwr(num_clients=NUM_CLIENTS, alpha=0.5, seed=42):\n",
        "    partitioner = DirichletPartitioner(\n",
        "        num_partitions=num_clients, partition_by=\"label\", alpha=alpha, seed=seed\n",
        "    )\n",
        "    fds = FederatedDataset(dataset=\"mnist\", partitioners={\"train\": partitioner})\n",
        "    federated_data = {f\"client_{i}\": fds.load_partition(i) for i in range(num_clients)}\n",
        "    return fds, federated_data  # Trả về cả fds và dữ liệu phân vùng\n",
        "\n",
        "def load_data(num_clients: int):\n",
        "    _, federated_data = split_mnist_dirichlet_flwr(num_clients)\n",
        "    client_loaders = {}\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        images, labels = federated_data[f\"client_{i}\"]\n",
        "        images = torch.tensor(images).unsqueeze(1).float() / 255.0\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "        \n",
        "        dataset = TensorDataset(images, labels)\n",
        "        client_loaders[i] = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    return client_loaders\n",
        "\n",
        "class SimpleClientManager(ClientManager):\n",
        "    def __init__(self) -> None:\n",
        "        self.clients: Dict[str, ClientProxy] = {}\n",
        "        self._cv = threading.Condition()\n",
        "        self.seed = 0 # cài đặt seed để fix client tham gia mỗi round\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.clients)\n",
        "\n",
        "    def num_available(self) -> int:\n",
        "        return len(self)\n",
        "\n",
        "    def wait_for(self, num_clients: int, timeout: int = 86400) -> bool:\n",
        "        with self._cv:\n",
        "            return self._cv.wait_for(\n",
        "                lambda: len(self.clients) >= num_clients, timeout=timeout\n",
        "            )\n",
        "\n",
        "    def register(self, client: ClientProxy) -> bool:\n",
        "        if client.cid in self.clients:\n",
        "            return False\n",
        "\n",
        "        self.clients[client.cid] = client\n",
        "        with self._cv:\n",
        "            self._cv.notify_all()\n",
        "\n",
        "        return True\n",
        "\n",
        "    def unregister(self, client: ClientProxy) -> None:\n",
        "        if client.cid in self.clients:\n",
        "            del self.clients[client.cid]\n",
        "\n",
        "            with self._cv:\n",
        "                self._cv.notify_all()\n",
        "\n",
        "    def all(self) -> Dict[str, ClientProxy]:\n",
        "        return self.clients\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "        num_clients: int,\n",
        "        min_num_clients: Optional[int] = None,\n",
        "        criterion: Optional[Criterion] = None,\n",
        "    ) -> List[ClientProxy]:\n",
        "\n",
        "        if min_num_clients is None:\n",
        "            min_num_clients = num_clients\n",
        "        self.wait_for(min_num_clients)\n",
        "        available_cids = list(self.clients)\n",
        "\n",
        "        if num_clients == 1:\n",
        "            sampled_cids = random.sample(available_cids, num_clients)\n",
        "            return [self.clients[cid] for cid in sampled_cids]\n",
        "\n",
        "        sampled_cids = random.sample(available_cids, num_clients)\n",
        "        self.seed +=1\n",
        "        return [self.clients[cid] for cid in sampled_cids]\n",
        "\n",
        "def main():\n",
        "    client_data = load_data(NUM_CLIENTS)\n",
        "    client_manager = SimpleClientManager()\n",
        "    def client_fn(cid: str): return PrivacyClient(cid, Net(), client_data[int(cid)])\n",
        "    strategy = FedAvg_Privacy(min_available_clients=NUM_CLIENTS, min_fit_clients=K_CLIENTS,client_manager =client_manager)\n",
        "    history = fl.simulation.start_simulation(client_fn=client_fn, num_clients=NUM_CLIENTS, config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS), strategy=strategy)\n",
        "    if hasattr(strategy, 'metrics_history'):\n",
        "        plot_training_metrics(strategy.metrics_history)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
